{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "92\n",
      "10\n",
      "106\n",
      "97\n",
      "9\n",
      "100\n",
      "91\n",
      "9\n",
      "82\n",
      "69\n",
      "13\n",
      "105\n",
      "97\n",
      "8\n",
      "91\n",
      "78\n",
      "13\n",
      "104\n",
      "90\n",
      "14\n",
      "145\n",
      "122\n",
      "23\n",
      "140\n",
      "112\n",
      "28\n",
      "120\n",
      "106\n",
      "14\n",
      "137\n",
      "110\n",
      "27\n",
      "99\n",
      "86\n",
      "13\n",
      "71\n",
      "60\n",
      "11\n",
      "86\n",
      "75\n",
      "11\n",
      "60\n",
      "52\n",
      "8\n",
      "107\n",
      "85\n",
      "22\n",
      "88\n",
      "74\n",
      "14\n",
      "143\n",
      "120\n",
      "23\n",
      "131\n",
      "111\n",
      "20\n",
      "128\n",
      "109\n",
      "19\n",
      "160\n",
      "132\n",
      "28\n",
      "137\n",
      "116\n",
      "21\n",
      "146\n",
      "107\n",
      "39\n",
      "73\n",
      "64\n",
      "9\n",
      "107\n",
      "86\n",
      "21\n",
      "88\n",
      "81\n",
      "7\n",
      "109\n",
      "95\n",
      "14\n",
      "112\n",
      "95\n",
      "17\n",
      "114\n",
      "93\n",
      "21\n",
      "150\n",
      "121\n",
      "29\n",
      "173\n",
      "134\n",
      "39\n",
      "129\n",
      "102\n",
      "27\n",
      "144\n",
      "122\n",
      "22\n",
      "100\n",
      "87\n",
      "13\n",
      "126\n",
      "105\n",
      "21\n",
      "94\n",
      "76\n",
      "18\n",
      "124\n",
      "111\n",
      "13\n",
      "113\n",
      "95\n",
      "18\n",
      "88\n",
      "74\n",
      "14\n",
      "75\n",
      "65\n",
      "10\n",
      "133\n",
      "120\n",
      "13\n",
      "81\n",
      "62\n",
      "19\n",
      "120\n",
      "103\n",
      "17\n",
      "101\n",
      "83\n",
      "18\n",
      "156\n",
      "128\n",
      "28\n",
      "103\n",
      "79\n",
      "24\n",
      "97\n",
      "81\n",
      "16\n",
      "119\n",
      "94\n",
      "25\n",
      "132\n",
      "107\n",
      "25\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "\n",
    "#====================등급파일 어휘 고르기================================\n",
    "\n",
    "all_grade_word_pos = [] #등급파일의 어휘-품사\n",
    "exc_word_pos = [] #어휘-품사가 같은데 등급이 다른 거\n",
    "with open('\\\\Users\\\\82107\\\\Desktop\\\\p2\\\\test\\\\어휘품사.txt','r',encoding='utf-8') as word_pos_file:\n",
    "    word_pos_lines = word_pos_file.readlines()\n",
    "    for word_pos_line in word_pos_lines:\n",
    "        if word_pos_line.split() in all_grade_word_pos:\n",
    "            exc_word_pos.append(word_pos_line.split())\n",
    "        all_grade_word_pos.append(word_pos_line.split())\n",
    "        \n",
    "grade_word_pos = [] #exc_word_pos 제외하고 (중복 제거한)->엑셀 자체 등급파일의 어휘-품사(동형명등)\n",
    "for word_pos in all_grade_word_pos:\n",
    "    if word_pos not in exc_word_pos:\n",
    "        if word_pos[1] in ['EP','EF','EC','ETN','ETM']:\n",
    "            grade_word_pos.append(word_pos)\n",
    "\n",
    "#====================교재파일 어휘 고르기================================\n",
    "\n",
    "directory = os.listdir('\\\\Users\\\\82107\\\\Desktop\\\\p2\\\\test\\\\book_split')\n",
    "os.chdir('\\\\Users\\\\82107\\\\Desktop\\\\p2\\\\test\\\\book_split')\n",
    "for file in directory:\n",
    "    all_book_word_pos = [] #중복 제거한 교재의 모든 어휘-품사\n",
    "    with open(file,'r',encoding='utf-8') as book_word_pos_file:\n",
    "        lines = book_word_pos_file.readlines()\n",
    "        all_word_pos_lines = []# 중복 포함\n",
    "        for i in lines: all_word_pos_lines.append(i.strip())\n",
    "        word_pos_lines = []\n",
    "        count_list = [] #교재의 어휘-품사 숫자\n",
    "        for word_pos_line in all_word_pos_lines:\n",
    "            if word_pos_line not in word_pos_lines:\n",
    "                word_pos_lines.append(word_pos_line)\n",
    "                count_list.append(all_word_pos_lines.count(word_pos_line))\n",
    "        for word_pos_line in word_pos_lines:\n",
    "            all_book_word_pos.append(word_pos_line.split('/'))\n",
    "    \n",
    "    book_word_pos = [] #교재의 어휘-품사(동형명등)\n",
    "    for word_pos in all_book_word_pos:\n",
    "        if word_pos[1] in ['EP','EF','EC','ETN','ETM']:\n",
    "            book_word_pos.append(word_pos)\n",
    "    \n",
    "#====================================================================\n",
    "    \n",
    "    grade_csv = open('\\\\Users\\\\82107\\\\Desktop\\\\p2\\\\test\\\\word_grade.csv','r',encoding='utf-8')\n",
    "    rdr = csv.reader(grade_csv)\n",
    "    lines = [] #어휘 등급표 전체 line들\n",
    "    for line in rdr:\n",
    "        lines.append(line)\n",
    "    \n",
    "    with open('\\\\Users\\\\82107\\\\Desktop\\\\p2\\\\test\\\\except\\\\'+file[:-4]+'-except.txt','w',encoding='utf-8') as except_file: #교재에 포함된 어휘-품사가 같은데 등급이 다른 거\n",
    "        for word_pos in book_word_pos:\n",
    "            if word_pos in exc_word_pos:\n",
    "                except_file.write(str(word_pos[0])+'\\t'+str(word_pos[1])+'\\n')\n",
    "                \n",
    "    print(len(book_word_pos))         \n",
    "    final_file = open('\\\\Users\\\\82107\\\\Desktop\\\\p2\\\\test\\\\result\\\\'+file[:-4]+'-result.txt','w',encoding='utf-8')\n",
    "    finished_word = []\n",
    "    for g_word_pos in grade_word_pos:\n",
    "        grade_pos, grade_word = g_word_pos[1], g_word_pos[0]\n",
    "        pos_reg = re.compile(grade_pos)\n",
    "        for word_pos in book_word_pos:\n",
    "            book_pos, book_word = word_pos[1],word_pos[0]\n",
    "            if grade_pos == book_pos and grade_word == book_word:\n",
    "                if word_pos not in finished_word and word_pos not in exc_word_pos:\n",
    "                    final_file.write(book_word+'/'+book_pos)\n",
    "                    for i in lines[all_grade_word_pos.index([grade_word,grade_pos])]:\n",
    "                        final_file.write('\\t'+i)\n",
    "                    final_file.write('\\n')\n",
    "                    finished_word.append(word_pos)\n",
    "            elif grade_pos == 'ET' and book_pos in ['ETN','ETM']:\n",
    "                if grade_word == book_word:      \n",
    "                    if word_pos not in finished_word and word_pos not in exc_word_pos:\n",
    "                        final_file.write(book_word+'/'+book_pos)\n",
    "                        for i in lines[all_grade_word_pos.index([grade_word,grade_pos])]:\n",
    "                            final_file.write('\\t'+i)\n",
    "                        final_file.write('\\n')\n",
    "                        finished_word.append(word_pos)\n",
    "    final_file.close()\n",
    "    \n",
    "    print(len(finished_word))\n",
    "    \n",
    "    i = 0  \n",
    "    with open('\\\\Users\\\\82107\\\\Desktop\\\\p2\\\\test\\\\else\\\\'+file[:-4]+'-else.txt','w',encoding='utf-8') as else_file: #노가다 검수\n",
    "        for word_pos in book_word_pos:\n",
    "            if word_pos not in finished_word:\n",
    "                else_file.write(str(word_pos[0])+'\\t'+str(word_pos[1])+'\\n')\n",
    "                i += 1\n",
    "    print(i)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
